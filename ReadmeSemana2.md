# Semana 2 (Proyecto Final taxis_NY)

<div style="display: flex; justify-content:space-between;">
        <img src="src/WhatsApp Image 2024-11-14 at 13.42.47.jpeg" alt="Imagen 3" width="800" height="350">
</div>
</br>

##  Indice.

1.Descripcion general

2.Diagrama E-R

3.Diccionario de Datos

4.Pipelines ETL

5.Flujo de trabajo

6.Demostracion de carga incremental

7.Minimun Viable Product (MVP) de dashboard

8.Herramientas para el Stack Tecnlogico


## 1. Descripcion General.

Durante la segunda semana del proyecto, el equipo se dedic贸 a las labores de ingenier铆a de datos. Se procedi贸 a la extracci贸n de datos desde m煤ltiples fuentes, las cuales fueron sometidas a un riguroso proceso de depuraci贸n para garantizar su calidad. Tambien se ha impulsado la implementaci贸n de un almac茅n de datos (Data Warehouse) automatizado, aprovechando las capacidades de la plataforma Google Cloud Platform (GCP). Mediante la integraci贸n de servicios como Google Storage, Google BigQuery y la orquestaci贸n de Apache Airflow, se ha dise帽ado una infraestructura robusta para la carga, transformaci贸n y an谩lisis de grandes vol煤menes de datos. Esta soluci贸n innovadora permite procesar informaci贸n de manera eficiente y escalable, optimizando los flujos de trabajo de ETL.


## 2. Diagrama Entidad Relacion (DER).

Con el prop贸sito de visualizar la arquitectura de la informaci贸n, se ha elaborado un diagrama Entidad-Relaci贸n. El mismo define las relaciones entre las distintas tablas en el Data Warehouse. Este modelo asegura que los datos se organicen de manera eficiente y facilita la realizaci贸n de consultas complejas en BigQuery.

## 3. Diccionario de Datos.

Con el objetivo de fomentar una interpretaci贸n un铆voca de los datos, se ha creado un diccionario de datos que sirva como referente terminol贸gico, al cual se puede acceder mediante el siguiente link: [Diccionariodedatos.md](https://github.com/jdbaquero84/NY_cabs_consultant/blob/ae71847920692e146845ccd6fe6ac24f9a926a62/Diccionariodedatos.md)

## 4. Pipelines ETL. 

La presente ilustraci贸n gr谩fica detalla el flujo de trabajo ETL desarrollado. Se observa una pluralidad de fuentes de datos que, mediante la aplicaci贸n de t茅cnicas de limpieza y transformaci贸n, son integradas en la base de datos de Google BigQuery.


<div style="display: flex; justify-content:space-between;">
        <img src="src/WhatsApp Image 2024-11-14 at 13.42.02.jpeg" alt="Imagen 3" width="800" height="350">
</div>
</br>


## 5. Flujo de Trabajo.

El flujo de trabajo propuesto se fundamenta en el empleo de tecnolog铆as para el manejo y an谩lisis de grandes vol煤menes de datos. Inicialmente, se llevar谩 a cabo la ingesta y transformaci贸n de los datos, los cuales ser谩n almacenados en un Data Warehouse. Posteriormente, se aplicar谩n t茅cnicas de Machine Learning para desarrollar modelos predictivos y descriptivos. Finalmente, se utilizar谩 la herramienta PowerBI para construir un panel de control interactivo, que permitir谩 visualizar los resultados de manera clara y concisa.

<div style="display: flex; justify-content:space-between;">
        <img src="src/WhatsApp Image 2024-11-14 at 13.41.39.jpeg" alt="Imagen 3" width="800" height="350">
</div>
</br>


## 6. Carga incremental automatizada.

Se presentar谩 una descripci贸n detallada de los procesos llevados a cabo. Se invita al lector a complementar esta informaci贸n visualizando el video adjunto = ...

## 7. Minimun Viable Product (MVP) de dashboard.

Se elabor贸 un prototipo de tablero de control que materializ贸 visualmente una selecci贸n de los insights m谩s relevantes extra铆dos del an谩lisis de datos. Este MVP proporcion贸 un primer acercamiento a las funcionalidades y est茅tica del producto definitivo.

## 8. Herramientas para el Stack Tecnol贸gico

### Exploraci贸n y Transformaci贸n de Datos (EDA y ETL)

- **VSCode**
  - Visual Studio Code es un entorno de desarrollo ligero, modular y con soporte para m煤ltiples lenguajes y extensiones. Su integraci贸n con herramientas de control de versiones y sus capacidades de depuraci贸n lo convierten en una herramienta esencial para desarrollar scripts de an谩lisis y transformaci贸n de datos en Python.

- **Python**
  - Python es el lenguaje principal en ciencia de datos debido a su simplicidad, extensa comunidad y amplia colecci贸n de bibliotecas de an谩lisis y machine learning. Su versatilidad permite tanto la manipulaci贸n de datos como el desarrollo de modelos de machine learning y la creaci贸n de visualizaciones, todo en un mismo entorno.

- **Pandas y NumPy**
  - Pandas es la biblioteca de referencia para la manipulaci贸n y an谩lisis de datos en Python. Por otro lado, NumPy es una biblioteca fundamental para el c谩lculo num茅rico en Python, especialmente en el 谩mbito de la ciencia de datos, la estad铆stica y el machine learning.

- **Matplotlib y Seaborn**
  - Estas bibliotecas de visualizaci贸n son fundamentales para explorar patrones y relaciones en los datos durante el an谩lisis exploratorio de datos (EDA). Matplotlib ofrece una personalizaci贸n detallada de gr谩ficos, mientras que Seaborn facilita la creaci贸n de gr谩ficos estad铆sticos atractivos y permite obtener insights visuales que ayudan a tomar decisiones informadas durante el procesamiento y an谩lisis.

 - **Google Colab** 
    - Google Colab es una plataforma ideal para el trabajo colaborativo en ciencia de datos y machine learning, ya que permite a equipos desarrollar proyectos de manera conjunta y eficiente. Al estar basado en la nube, facilita que varios miembros del equipo accedan y trabajen en el mismo notebook desde cualquier ubicaci贸n, simplemente utilizando un navegador web.

### Machine Learning

- **Scikit-Learn**

  - Scikit-Learn es una biblioteca de machine learning en Python que ofrece algoritmos eficientes y bien documentados para clasificaci贸n, regresi贸n y clustering, entre otros. Su simplicidad y versatilida permite construir y evaluar modelos de manera r谩pida y confiable. Adem谩s, es una biblioteca abierta y altamente extensible, lo cual la convierte en una herramienta ideal para nuestra soluci贸n de machine learning.

### Visualizaci贸n

- **Power BI**

  - Es una herramienta de visualizaci贸n y an谩lisis de datos de Microsoft que permite transformar datos en informaci贸n visual e interactiva. Herramienta conocida y ampliamente usada por nuestro equipo ya que permite la creaci贸n de dashboards, reportes y an谩lisis de forma vers谩til.

### Automatizaci贸n

- **Google Cloud Platform (GCP)**
  - GCP proporciona una infraestructura escalable y segura que permite almacenar y procesar grandes vol煤menes de datos sin necesidad de invertir en infraestructura f铆sica. Su conjunto de servicios administrados que pretendemos utilizar, como BigQuery para an谩lisis de datos y Cloud Storage para almacenamiento, reduce significativamente los costos y la complejidad de manejar grandes vol煤menes de datos. Adem谩s, GCP ofrece herramientas avanzadas de machine learning y soporte para entornos h铆bridos y multicloud, lo cual facilita la integraci贸n y despliegue de aplicaciones en un ecosistema m谩s amplio.

- **GitHub**
  - GitHub es esencial para el control de versiones de c贸digo, permitiendo un trabajo colaborativo y organizado en el equipo. Su capacidad para gestionar versiones de scripts y documentaci贸n asegura que cualquier cambio pueda ser rastreado y revertido si es necesario, lo cual es crucial para mantener la integridad del proyecto y facilitar la colaboraci贸n entre los miembros del equipo.

- **Docker**
  - Docker permite crear entornos de ejecuci贸n consistentes, independientemente del sistema operativo o las dependencias de software. Esto garantiza que los scripts de ETL, los modelos de machine learning y las aplicaciones puedan desplegarse de manera uniforme y reproducible. La contenedorizaci贸n tambi茅n simplifica la escalabilidad y el mantenimiento de los entornos, especialmente en proyectos que requieren m煤ltiples versiones de dependencias.

- **Apache Airflow**
  - Airflow es una herramienta de orquestaci贸n de flujos de trabajo que permite automatizar los procesos ETL y el despliegue de modelos de machine learning de forma estructurada y controlada. Su capacidad para gestionar dependencias, programar tareas y monitorear el estado de los procesos hace que sea una herramienta esencial en entornos de producci贸n. Esto garantiza que las tareas se ejecuten en el orden correcto y proporciona alertas en caso de errores o fallos en el flujo de trabajo.

### Conclusi贸n
Cada una de estas herramientas cumple un rol espec铆fico dentro del flujo de trabajo del proyecto, desde la ingesta de datos hasta la automatizaci贸n de tareas. Al combinar estas tecnolog铆as, se crea una infraestructura robusta, eficiente y escalable, que no solo facilita el an谩lisis y el procesamiento de grandes vol煤menes de datos, sino que tambi茅n asegura que el proceso sea sostenible, reproducible y f谩cil de gestionar a largo plazo.
